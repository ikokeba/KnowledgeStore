---
tags:
  - AI
  - 機械学習
  - データサイエンス
  - 研究
  - 自然言語処理
  - アテンションメカニズム
  - 記事
  - RNN
  - 理論
  - Python
既読・整理済み: false
---

---
---
# asap

**Author:** @asap2650
**Date:** 2025-08-02T18:17:12.000Z
**URL:** https://x.com/asap2650/status/1951708878888202326
**Replies:** 1件（本人の返信）

---

## Content

https://arxiv.org/abs/2507.23632

「Softmax Attentionは、実は無限個のRNNの足し合わせで表現できる」
「Liner AttentionはSoftmax Attentionをテイラー展開した時の1次近似」

この視点はなかった。

## Links

- [https://arxiv.org/abs/2507.23632](https://t.co/G0iGcnV6uq)
- [arxiv.orgOn the Expressiveness of Softmax Attention: A Recurrent Neural...Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main...](https://t.co/G0iGcnV6uq)

## Replies (返信一覧)

### Reply 1 (返信数情報のみ)
**Info:** 返信数: 1件 (詳細はツイートページで確認してください)
**Note:** ブックマークページでは返信内容を表示できません。詳細はツイートページをご確認ください。

---

*Exported at: 8/20/2025, 10:35:22 AM*
