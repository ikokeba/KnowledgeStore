---
tags:
  - 記事
  - MoE
  - 技術解説
  - 機械学習
  - チュートリアル
  - llama.cpp
  - モデル
  - AI
  - 量子化
  - ローカルLLM
既読・整理済み: false
---

# もみじあめ

**Author:** @momijiame
**Date:** 2025-07-26T04:03:00.000Z
**URL:** https://x.com/momijiame/status/1948957197909549283

---

## Content

最近はローカル LLM について調べてるんだけど 100B を超えるモデルを動かすやり方だと unsloth の量子化モデルとドキュメントが参考になる。直近の大きなモデルは基本的に MoE だから、llama.cpp で MoE のレイヤーだけメインメモリにオフロードさせるやり方が書いてある。

## Links

- [docs.unsloth.aiQwen3-2507 | Unsloth DocumentationRun Qwen3-235B-A22B-Thinking-2507 and Qwen3-235B-A22B-Instruct-2507 locally on your device!](https://t.co/xj56BcsXkq)

---

*Exported at: 7/28/2025, 3:58:10 PM*
