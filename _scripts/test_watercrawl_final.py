#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WaterCrawl æœ€çµ‚ãƒ†ã‚¹ãƒˆ

WaterCrawl APIã®æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½œæˆæ—¥: 2025-01-27
"""

import os
import time
from watercrawl import WaterCrawlAPIClient

def test_watercrawl_final():
    """WaterCrawlã®æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ"""
    
    # APIã‚­ãƒ¼ã‚’ç¢ºèª
    api_key = os.getenv('WATERCRAWL_API_KEY')
    if not api_key:
        print("âŒ WATERCRAWL_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    try:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        client = WaterCrawlAPIClient(api_key=api_key)
        print("âœ… ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®URL
        test_url = "https://example.com"
        print(f"\nğŸ”— ãƒ†ã‚¹ãƒˆURL: {test_url}")
        
        # æ–¹æ³•1: scrape_urlã‚’æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä½¿ç”¨
        print("\nğŸ“‹ æ–¹æ³•1: scrape_urlï¼ˆæ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰")
        try:
            result = client.scrape_url(
                url=test_url,
                page_options=None,
                plugin_options=None,
                sync=True,
                download=True
            )
            print(f"scrape_urlçµæœ: {result}")
            if result:
                print(f"çµæœã®å‹: {type(result)}")
                print(f"çµæœã®å±æ€§: {dir(result)}")
                
                # è¦ç´„ã‚„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèª
                if hasattr(result, 'summary'):
                    print(f"è¦ç´„: {result.summary}")
                if hasattr(result, 'text'):
                    print(f"ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰: {result.text[:200]}...")
                if hasattr(result, 'title'):
                    print(f"ã‚¿ã‚¤ãƒˆãƒ«: {result.title}")
            
        except Exception as e:
            print(f"æ–¹æ³•1ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–¹æ³•2: create_crawl_requestã‚’æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä½¿ç”¨
        print("\nğŸ“‹ æ–¹æ³•2: create_crawl_requestï¼ˆæ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰")
        try:
            crawl_request = client.create_crawl_request(url=test_url)
            print(f"ã‚¯ãƒ­ãƒ¼ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆæˆåŠŸ: {crawl_request}")
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDã‚’å–å¾—
            if hasattr(crawl_request, 'id'):
                request_id = crawl_request.id
                print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆID: {request_id}")
                
                # çµæœã‚’ç›£è¦–
                print("çµæœã‚’ç›£è¦–ä¸­...")
                for i in range(10):  # æœ€å¤§10å›è©¦è¡Œ
                    time.sleep(3)  # 3ç§’å¾…æ©Ÿ
                    
                    try:
                        results = client.get_crawl_request_results(request_id)
                        if results and len(results) > 0:
                            print(f"çµæœå–å¾—æˆåŠŸ: {results}")
                            # æœ€åˆã®çµæœã®è©³ç´°ã‚’è¡¨ç¤º
                            first_result = results[0] if isinstance(results, list) else results
                            print(f"çµæœã®å‹: {type(first_result)}")
                            print(f"çµæœã®å±æ€§: {dir(first_result)}")
                            break
                        else:
                            print(f"è©¦è¡Œ {i+1}: çµæœãªã—")
                    except Exception as e:
                        print(f"è©¦è¡Œ {i+1}: ã‚¨ãƒ©ãƒ¼ - {e}")
                
        except Exception as e:
            print(f"æ–¹æ³•2ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–¹æ³•3: ãƒãƒƒãƒã‚¯ãƒ­ãƒ¼ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä½¿ç”¨
        print("\nğŸ“‹ æ–¹æ³•3: ãƒãƒƒãƒã‚¯ãƒ­ãƒ¼ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆæ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰")
        try:
            batch_request = client.create_batch_crawl_request(urls=[test_url])
            print(f"ãƒãƒƒãƒã‚¯ãƒ­ãƒ¼ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆæˆåŠŸ: {batch_request}")
            
        except Exception as e:
            print(f"æ–¹æ³•3ã‚¨ãƒ©ãƒ¼: {e}")
            
    except Exception as e:
        print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    test_watercrawl_final() 