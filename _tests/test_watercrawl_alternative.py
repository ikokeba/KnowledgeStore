#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WaterCrawl ä»£æ›¿ãƒ†ã‚¹ãƒˆ

WaterCrawl APIã®ä»£æ›¿çš„ãªä½¿ç”¨æ–¹æ³•ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½œæˆæ—¥: 2025-01-27
"""

import os
import time
from watercrawl import WaterCrawlAPIClient

def test_watercrawl_alternative():
    """WaterCrawlã®ä»£æ›¿çš„ãªä½¿ç”¨æ–¹æ³•ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    # APIã‚­ãƒ¼ã‚’ç¢ºèª
    api_key = os.getenv('WATERCRAWL_API_KEY')
    if not api_key:
        print("âŒ WATERCRAWL_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    try:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        client = WaterCrawlAPIClient(api_key=api_key)
        print("âœ… ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®URL
        test_url = "https://example.com"
        print(f"\nğŸ”— ãƒ†ã‚¹ãƒˆURL: {test_url}")
        
        # æ–¹æ³•1: create_crawl_requestã‚’ä½¿ç”¨
        print("\nğŸ“‹ æ–¹æ³•1: create_crawl_requestã‚’ä½¿ç”¨")
        try:
            crawl_request = client.create_crawl_request(
                urls=[test_url],
                name="Test Crawl"
            )
            print(f"ã‚¯ãƒ­ãƒ¼ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆæˆåŠŸ: {crawl_request}")
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDã‚’å–å¾—
            if hasattr(crawl_request, 'id'):
                request_id = crawl_request.id
                print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆID: {request_id}")
                
                # çµæœã‚’ç›£è¦–
                print("çµæœã‚’ç›£è¦–ä¸­...")
                for i in range(10):  # æœ€å¤§10å›è©¦è¡Œ
                    time.sleep(2)  # 2ç§’å¾…æ©Ÿ
                    
                    try:
                        results = client.get_crawl_request_results(request_id)
                        if results:
                            print(f"çµæœå–å¾—æˆåŠŸ: {results}")
                            break
                        else:
                            print(f"è©¦è¡Œ {i+1}: çµæœãªã—")
                    except Exception as e:
                        print(f"è©¦è¡Œ {i+1}: ã‚¨ãƒ©ãƒ¼ - {e}")
                
        except Exception as e:
            print(f"æ–¹æ³•1ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–¹æ³•2: ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ã‚’è©¦è¡Œ
        print("\nğŸ“‹ æ–¹æ³•2: ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–")
        try:
            client.init_session()
            print("ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–æˆåŠŸ")
            
            # å†åº¦scrape_urlã‚’è©¦è¡Œ
            result = client.scrape_url(test_url)
            print(f"scrape_urlçµæœ: {result}")
            
        except Exception as e:
            print(f"æ–¹æ³•2ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–¹æ³•3: åˆ©ç”¨å¯èƒ½ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç¢ºèª
        print("\nğŸ“‹ æ–¹æ³•3: åˆ©ç”¨å¯èƒ½ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç¢ºèª")
        try:
            requests_list = client.get_crawl_requests_list()
            print(f"åˆ©ç”¨å¯èƒ½ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {requests_list}")
        except Exception as e:
            print(f"æ–¹æ³•3ã‚¨ãƒ©ãƒ¼: {e}")
            
    except Exception as e:
        print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    test_watercrawl_alternative() 